[
  {
    "path": "posts/2023-06-03-searching-for-user-sessions/",
    "title": "Searching for User Sessions",
    "description": "How to assign Session ID's to group event data.",
    "author": [
      {
        "name": "Lukas Gröninger",
        "url": {}
      }
    ],
    "date": "2023-06-03",
    "categories": [
      "R",
      "SQL"
    ],
    "contents": "\n\nContents\nThe concept of a User Session\nSearching in an online marketplace\nCalculating a session_id in R\nCalculating a session_id in SQL\nConclusion\n\nThis blog post will show how we can calculate user session_id’s and group event data.\nIt will be illustrated with the help of R and SQL.\nThe concept of a User Session\nI’m working in the realm of (online) Data Analytics. That means I’m interested in analyzing\nthe behaviour of people who use our products and how they interact with it.\nA helpful concept in this domain is that of a User Session.\nMaybe u have heard about it, maybe not. But here u go:\n\nA session is a group of interactions between a user and an application that take place within a given timeframe. A single session can contain multiple activities (such as page views, events, social interactions, and e-commerce transactions)1\n\nThis concept leads to further metrics that are of interest such as the number of sessions, the average\nduration of a session or the probability of a user/customer performing certain actions within a session.\nLet’s take a simple example to illustrate this idea and narrow it down.\nSearching in an online marketplace\nImagine I’m searching for a new bike and visit Kleinanzeigen.de\n(a german classifieds site where u can buy and sell pretty much anything).\nTypically I enter the thing of interest (here a bike) and get already proposals (see screenshot).\nAs soon as I click on any of these, the results load and are\ndisplayed. This typically triggers an event that a user has done a search.\nSearching for a bikeHowever for the process of finding a bike for me, it is probably not sufficient\njust to specify that I search for a bike.\nI will also want to include the location and possibly apply other filter options such as a price range.\nFurther Filter OptionsAfter having entered the location of e.g. Hamburg-Eimsbüttel and defined a price range from\nlet’s say 200 to 500 €,\nthis resulted in multiple triggered search events.\nBut in a more general sense, we might argue that these events belong together and result\nin one search (session).\nAt this point we want to group these searches together and assign one session_id.\nIn order to make it easier to comprehend, I am creating dummy data of how\nthese searches may be stored.\nCalculating a session_id in R\n\n\n# required libraries\nlibrary(tidyverse)\nlibrary(dbplyr)\n\n\n\nAt first, I create a dataframe with 3 columns:\na unique search_id\na user_id to define who has done the search\na timestamp created_at to indicate the time when the search has been done\n\n\n# Create dummy data\nsearches <- tibble(search_id = 1:10, \n                   user_id = rep(c(1,2), each = 5), \n                   created_at = ymd_hms(\n                        c(\"2023-06-03 08:10:00\", \"2023-06-03 08:10:08\",\n                          \"2023-06-03 08:11:21\", \"2023-06-03 08:11:24\",\n                          \"2023-06-03 08:11:33\", \"2023-06-03 08:10:48\",\n                          \"2023-06-03 08:11:05\", \"2023-06-03 08:11:11\",\n                          \"2023-06-03 08:12:13\", \"2023-06-03 08:12:20\"))) |> \n  arrange(user_id, created_at)\n\n\n\n\n\n\n\nWe’ve got 10 individual searches of 2 users. Each user has done 5 searches and\nwe want to know how to assign session_id’s to these searches.\nMaybe someone in our team has already done some research and came to the conclusion that it is\nbest to define the timeframe of a search session to be 10 seconds.\nThis means if a subsequent search of a user is done within 10 seconds, these\nbelong to the same search session_id.\nTherefore we want to first sort our data based on user_id and created_at.\nThen we can calculate the time difference between searches.\nIn base R we’ve got the nice function diff which as the name suggests calculates the\nlagged difference.\n\n\ndiff(c(1, 1, 3, 4, 2))\n\n\n[1]  0  2  1 -2\n\nWe have to pad the first value to get a vector of the same length.\n\n\nsearches |> \n  mutate(time_dif = c(0, diff(created_at)),\n         time_dif_reset = if_else(time_dif >= 10, TRUE, FALSE),\n         user_reset = if_else(c(1, diff(user_id)) != 0, TRUE, FALSE))\n\n\n\n\n\n\n\nWe added a boolean variable indicating whether the time difference between the\nfollowing search is greater than 10 seconds. If this is the case the session is\n“reset” and starts again.\nWhat we also have to account for is the fact that a new session starts for every\nuser. This can be achieved in a similar way.\nHere we pad the first value of user_reset with a 1 because we want to start our session_id variable with 1.\nWhat is now left is to combine these 2 time_dif_reset and user_reset variables\ninto a general reset variable (time_dif_reset OR user_reset). The cumulative sum of this variable will then give us our\ndesired session_id.\n\n\nsearches |> \n  mutate(time_dif = c(0, diff(created_at)),\n         time_dif_reset = if_else(time_dif >= 10, TRUE, FALSE),\n         user_reset = if_else(c(1, diff(user_id)) != 0, TRUE, FALSE),\n         reset = if_else(time_dif_reset | user_reset, TRUE, FALSE),\n         session_id = cumsum(reset)) \n\n\n\n\n\n\n\nThis tells us now that the first user has done 2 search sessions (session_id 1 and 2)\nand the second user has done 3 search sessions (session_id 3, 4 and 5).\nWe would probably not divide this calculation into several steps, but\ncombine them into a function.\n\n\n# Put it into a function\ncreate_session_id <- function(created_at, user_id, time_dif = 10) {\n  return(cumsum(c(0, diff(created_at)) >= time_dif | \n                c(1, diff(user_id)) != 0))\n}\n\nsearches |> \n  mutate(session_id = create_session_id(created_at, user_id))\n\n\n\nCalculating a session_id in SQL\nNow we also look at how this can be accomplished in SQL. We’ll\ncreate an in memory SQLite database table with our searches data.\n\n\ncon <- DBI::dbConnect(RSQLite::SQLite(), dbname = \":memory:\")\n\n# Fill this with our searches data\ncopy_to(con, searches)\n\n# SQL query to create session_id's\ntbl(con, \n    sql(\n        \"\n         with \n         searches_prep as (\n            select \n                search_id,\n                user_id,\n                created_at,\n                case when \n                    created_at - lag(created_at) over \n                        (partition by user_id order by created_at) >= 10 \n                    or row_number() over \n                        (partition by user_id order by created_at) = 1\n                    then 1 \n                    else 0 \n                end as reset\n            from searches) \n         \n         select \n            search_id,\n            user_id,\n            created_at,\n            reset,\n            sum(reset) over (order by user_id, created_at) as session_id\n         from searches_prep\n        \"\n        )) \n\n\n# Source:   SQL [10 x 5]\n# Database: sqlite 3.38.5 [:memory:]\n   search_id user_id created_at reset session_id\n       <int>   <dbl>      <dbl> <int>      <int>\n 1         1       1 1685779800     1          1\n 2         2       1 1685779808     0          1\n 3         3       1 1685779881     1          2\n 4         4       1 1685779884     0          2\n 5         5       1 1685779893     0          2\n 6         6       2 1685779848     1          3\n 7         7       2 1685779865     1          4\n 8         8       2 1685779871     0          4\n 9         9       2 1685779933     1          5\n10        10       2 1685779940     0          5\n\nHere we’re using a first CTE to create our reset variable similar to the way we have\ndefined it in R. We calculate the lagged difference by user_id and assign a 1 if\nit exceeds 10 seconds and for every first row of\na new user we assign a 1 as well.\nIn the bottom query we calculate the cumulative sum that gives us our session_id.\nConclusion\nWe have looked at the important concept of User Sessions and at the logic of\nhow this can be calculated. The implementation was demonstrated with R as well\nas with SQL and focused on the example of Searches done in an Online Marketplace.\nThis was a basic example and you would often add further conditions for specific cases.\nThese Session ID’s can be viewed as a cornerstone on which to build\nfurther metrics of interest like the average session duration or a count of\nspecific events per session.\n\nsee https://auth0.com/docs/manage-users/sessions↩︎\n",
    "preview": "posts/2023-06-03-searching-for-user-sessions/img/searching.jpeg",
    "last_modified": "2023-06-03T19:55:16+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-05-finally-solving-the-mac-m1-odbc-issue/",
    "title": "Finally solving the Mac M1 odbc issue...",
    "description": "How I was finally able to connect to Snowflake from R using an odbc connection.",
    "author": [
      {
        "name": "Lukas Gröninger",
        "url": {}
      }
    ],
    "date": "2023-03-05",
    "categories": [
      "R",
      "Database connection"
    ],
    "contents": "\nFirst of all: I haven’t written a blog post in quite some time. This has probably something\nto do with me having started a new job last July. I’m working now as a Data Analyst at\nContainer xChange and that kept me rather busy.\nThis post is about finally being able to connect to our Snowflake datawarehouse\nafter several hours (and days) of browsing through github issues and stackoverflow threads.\nI almost gave up thinking “okay, I can use the snowflake python connector and won’t query directly from R”.\nBut let’s start from the beginning…\nWhen I started working at my new job, I received a Macbook. And to be more precise:\na Macbook Pro with an M1 processor. Previously I didn’t have any experience working\nwith MacOS operating systems, but I felt that the switch was not too difficult.\n\nIt was mainly about getting used to copy and paste with command + c/v and not control + c/v.\nThe analytical datawarehouse our team used, was AWS Redshift. Naturally I wanted to\nquery tables directly from R and loved how easy that was with the {DBI}, {bigrquery} and {dbplyr} packages\nat my last job.\nAfter some googling I found a way to connect to our datawarehous and select\ntables from specific schemas:\n\n\n# Redhshift connection\nlibrary(DBI)\n\ncon <- dbConnect(RPostgres::Redshift(),\n                 host = \"123.45.67.890\",\n                 dbname = \"db_name\",\n                 user = \"admin\",\n                 password = rstudioapi::askForPassword(\"Database password\"),\n                 port = 1234)\n\ntable_a <- tbl(con, Id(schema = \"xchange_123\", table = \"t_company\"))\n\n\n\nI was glad to find the RPostgres::Redshift() function that saved me from\ndownloading Redshift drivers and having to specify an odbc connection.\nFast forward to last November when we started to move our datawarehouse to dbt + Snowflake.\nUnfortunately thinking that it shouldn’t affect my workflows in R just because I was accessing\na different datawarehouse:\n\nWhen first searching for a way to connect to Snowflake from R I found a couple\nof posts where it looked fairly easy to do exactly that.\nOne was from Martin Stingl on his rstats-tips blog\n(highly recommended). And I also found a post on community.snowflake.com\ntitled How To Connect Snowflake with R/RStudio using ODBC driver on Windows/MacOS/Linux.\nThis too easy how to article consisted of 2 steps:\nCreate the ODBC DSN and test if it is working fine\nUse this sample code:\n\n\ninstall.packages(c(\"DBI\", \"dplyr\",\"dbplyr\",\"odbc\"))\nlibrary(DBI)\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(odbc)\nmyconn <- DBI::dbConnect(odbc::odbc(), \"SNOWFLAKE_DSN_NAME\", uid=\"USERNAME\", pwd='Snowflak123')\nmydata <- DBI::dbGetQuery(myconn,\"SELECT * FROM EMP\")\nhead(mydata)\n\n\n\nI tried to configure an odbc dsn and connect to Snowflake, but sadly I just couldn’t\nfind a way to do it. One of the good things about R is that things most of the\ntime just work out of the box (see RPostgres::Redshift()).\nBut here I faced many problems I couldn’t solve which led me to ask many questions like:\nWhich driver should I install and where should I save it?\nWhich packages to download for my M1 Macbook?\nIs aarch64 and arm64 the same thing?\nHow to display hidden files? (SHIFT + CMD + .)\nWhat is the correct folder to place my .odbc.ini file?\nWhy doesn’t odbc::odbcListDrivers() list my drivers?\nWhat should be used - unixodbc or iODBC - and what is the difference?\nDoes it have sth. to do with homebrew?\nWhat should I export to $PATH?\nWhat are symlinks and how to use them?\nWhere lies the root of the problem?\nWhen browsing through the web I felt somewhat reassured that apparently many people\nwith an M1 Macbook have experienced similar problems.\nI want to show some stops I did when browsing and searching for a solution.\nHere follows a short list of posts that dealt with\nwhat I thought was my issue:\nStackoverflow\nMac M1 Pro pyodbc Can’t open lib\nR odbc::odbcListDrivers() does not list driver\nUnable to connect to PostgreSQL with ODBC in R on MacOS\nRStudio/Posit Community\nTroubleshooting R <> Redshift connection via ODBC\nConnecting RStudio with Snowflake on MacBook M1\nCan’t open lib ‘/opt/homebrew/lib/libmsodbcsql.17.dylib’ from M1 Mac ARM64\nGithub\nCurrently there are 4 open issues in the r-dbi/odbc repository dealing with what I assume\nis the same issue. Just 2 days ago Sharon Wang has created a PR that addresses this as well (thanks a lot 😊).\nSource: https://github.com/r-dbi/odbc/issues?q=is%3Aissue+is%3Aopen+m1People have offered solutions that were helpful for some persons.\nThe sad thing however was that none of the posts have helped me with finding a solution.\nI have already come to terms with it and on the plus side highly improved my SQL skills\n😄. But today I sat down at my desk, had a coffee and checked again\nthese open odbc issues and found this new comment from Sharon Wang:\nSource: https://github.com/r-dbi/odbc/issues/482That somehow rang a bell which led me to this stackoverflow post\ntitled How do I install the ODBC driver for Snowflake successfully on an M1 Apple Silicon Mac?\nThe second most upvoted answer from Scott Brenstuhl\nfinally gave me the solution\nand I was able to connect to Snowflake.\nThis is basically what I had to do:\nUninstall R and Rstudio (brew uninstall packagename)\nInstall Rosetta (apples software for translating between architectures) and a homebrew version built with it\nUse this homebrew version to install odbc, R and Rstudio for x86_64 architecture\nInstall the snowflake driver (not the aarch one) in the default location (/opt/snowflake/snowflakeodbc/lib/universal/libSnowflake.dylib)\nModify the odbcinst.ini file under /usr/local/etc/odbcinst.ini\nModify the odbc.ini file under /usr/local/etc/odbc.ini\nModify the simba.snowflake.ini file (needs sudo command to change) under /opt/snowflake/snowflakeodbc/lib/universal/simba.snowflake.ini\nFinally connect with DBI::dbConnect(odbc::odbc(), \"Snowflake Driver\")\nIn the end, I’m glad I found a solution, although it took me far too long.\nHopefully this post may help other people to stumble upon the correct\ngithub/stackoverflow/positcommunity thread.\n\n\n\n",
    "preview": "posts/2023-03-05-finally-solving-the-mac-m1-odbc-issue/img/snowflake_cloud_comp.jpeg",
    "last_modified": "2023-06-03T13:04:20+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-25-surviving-from-scratch/",
    "title": "Surviving from scratch",
    "description": "How to model exponential survival probabilities using maximum likelihood from scratch",
    "author": [
      {
        "name": "Lukas Gröninger",
        "url": {}
      }
    ],
    "date": "2022-06-25",
    "categories": [
      "R",
      "survival analysis"
    ],
    "contents": "\n\nContents\nSummary of the paper\nThe statistical model\nApplication of the\nmodel\nDiscussion\n\nReproducing the\nresults of Feigl & Zelen\n\nThis post will be about surviving. To be more specific about\nprobabilities of surviving up to a time point \\(t\\). The field of survival analysis is\nquite interesting and although there is a strong connection to the field\nof epidemiology and medicine, survival can also stand for a variety of\nthings like mechanical parts “surviving” up to a certain point or\npredicting the point in time up to when housing loans are being paid\nback. I was glad to see a new package {censored} joining the\ntidymodels family incorporating different survival engines. Now we will\nturn to the beginnings of this field and see what one of the first\nregression models for survival data looked like.\n\nFor further information on survival analysis I can recommend the book by\nKleinbaum\nand Klein (2012).\nThe influential paper of Feigl and Zelen (1965) is about the estimation of\nexponential survival probabilities with concomitant information. That\nmeans they estimated survival distributions when the survival times are\nassumed to follow an exponential distribution where the parameters were\ninfluenced by a covariate. In their work they applied these methods on\ndata of deceased leukemia patients. I will give a summary of their paper\nand afterwards reproduce some of it in R using an alternative model.\nSummary of the paper\nThe paper by Feigl and\nZelen (1965)\npublished in the journal Biometrics deals with a statistical\nmodel for estimating the survival time of cancer patients. In\nparticular, they studied data from patients with leukaemia. Leukaemia\ndevelops in the cells of the bone marrow and is usually characterised by\na large increase in the number of white blood cells in the blood. During\nthe course of the disease, there is a constant increase in the\nconcentration of white blood cells, which indicates its severity. When\nit comes to predicting the survival time of patients, information on the\namount of white blood cells as well as other indicators of disease\nprogression is crucial. In their paper, Feigl and Zelen (1965) discuss a\nmethod for modelling survival times based on an exponential distribution\nwhose parameters depend on these indicators. It is therefore a\nparametric model with the white blood cell count as a covariate. In\naddition, the survival curves of two different patient groups are\ncompared. One group has Auer bodies in their cells and the other group\ndoes not.\nTo estimate the parameters of the model, they use the maximum\nlikelihood method. In what follows, I will discuss the statistical model\nand its methodology in more detail. I also describe in more detail the\ndata and their approach, as well as the conclusions the authors draw at\nthe end. What do their results mean and what alternative models could be\napplied in this area?\nThe statistical model\nThe objective of the two authors is to predict the probability of\nsurvival at a time \\(t\\) of patients\nwith leukaemia. The survival time variable follows an exponential\ndistribution whose parameters depend on variables such as the number of\nwhite blood cells. The probability density function of this survival\ntime can be represented as follows:\n\\[\n\\begin{aligned}\nf_i(t) &= \\lambda_i \\ exp(-\\lambda_i t) \\; \\; for \\; \\; t \\geqslant\n0  \\\\\n&= 0 \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\ \\ for\n\\; \\; t < 0\n\\end{aligned}\n\\]\nThe parameter \\(\\lambda\\) is then\ndependent on another variable \\(x\\)\nwhich determines the expected value of the survival time of the patient:\n\\(E(t_i) = \\frac{1}{\\lambda_i} = a + b\nx_i\\).\nThus, the parameter \\(\\lambda_i^{-1}\\) results from the equation\nresembling the classical linear regression of \\(a + bx_i\\). In order to estimate the two\nparameters \\(a\\) and \\(b\\), the maximum likelihood method is\napplied. The likelihood function is as follows:\n\\[\n\\begin{aligned}\nl(a, b) &= \\prod_{i=1}^n f_i(t_i) = \\prod_{i=1}^n \\lambda_i \\\nexp(-\\lambda_i t)  \\\\\n&= (\\prod_{i=1}^n (a + bx_i)^{-1})  (exp - \\sum_{i=1}^n t_i(a +\nbx_i)^{-1})\n\\end{aligned}\n\\]\nClassically, this function is logarithmised to facilitate its use. To\nestimate the two parameters, the partial derivatives \\(\\frac{\\partial L}{\\partial a}\\) and \\(\\frac{\\partial L}{\\partial b}\\) are now set\nto \\(0\\). The solution is obtained\nusing the Taylor series. Here, these two equations are expanded in first\norder terms. Then, the two parameters \\(\\hat\na\\) and \\(\\hat b\\) are\nestimated, respecting the constraint that \\(\\hat a + \\hat bx_i > 0\\) for all \\(x_i\\) since the parameter \\(\\lambda\\) can only be positive. Finally,\nthe estimated parameters are used to calculate the probability of\nsurvival of patient \\(i\\). It is\npresented as follows:\n\\[\nS_i(t) = \\int_t^{\\infty} f_i(\\xi) \\, d \\xi = exp (- \\lambda_i t) = exp \\\n[-t(a + bx_i)^{-1}]\n\\]\nAs a quality criterion for their estimation, the authors examined the\nexpected and estimated number of patient deaths in specific time\nintervals \\(t_i(p)\\). The intervals of\n\\(p\\) were chosen as quartile\nprobabilities \\(p = (0.25, 0.5,\n0.75)\\).\n\\[\n\\hat S_i[t_i(p)] = exp \\ [-t_i(p)(\\hat a + \\hat bx_i)^{-1}] = p\n\\]\nA \\(\\chi^2\\) test (in this case with\n2 degrees of freedom) can then be performed to compare the observed and\nestimated numbers for the specified intervals.\nApplication of the model\nNow that the theoretical statistical model has been presented, let us\nmove on to the application. As described earlier, this is uncensored\ndata on patients who died of leukaemia. The \\(33\\) patients are divided into two groups\naccording to the presence (AG Positive, \\(n =\n17\\)) or absence (AG Negative, \\(n =\n16\\)) of Auer bodies in the cytoplasm of their cells. Auer bodies\nor rods are found in about 30% of cases in myeloid leukaemia and may\nincrease the probability of a complete remission rate (Seymour and Estey 1993). The time to\ndeath of patients is given in weeks. The accompanying variable \\(x\\) is the white blood cell count (\\(wbc\\)) at diagnosis. This means that this\nvariable is considered to be independent of time.\nLooking at the data, it appears that patients in the GA positive\ngroup have a lower number of \\(wbc\\)\nand a significantly longer survival time on average than patients in the\nGA negative group. When estimating the parameters \\(a\\) and \\(b\\) for each group, the following values\nwere obtained using the presented method.\n\n\nTable 1: Maximum Likelihood parameters\n\n\nPatient Group\n\n\na\n\n\nb\n\n\nsd a\n\n\nsd b\n\n\nAG positive\n\n\n240\n\n\n-44\n\n\n95.5\n\n\n20.1\n\n\nAG negative\n\n\n30\n\n\n-3\n\n\n35.1\n\n\n8.2\n\n\nBy testing their model, they were able to show that the fit was\nadequate. The maximum difference between the observed survival\nprobabilities and the estimated mean is 12%. The most interesting\nfinding made by the authors concerns the division between the two\ngroups. For the first AG positive group, the survival probability\ndepends very strongly on the variable \\(wbc\\). For the second group, there is only\na weak relationship. This relationship is also very well reflected in\nthe different values of \\(b\\). The\nhigher \\(|b|\\) is, the greater the\ninfluence of the variable. The authors’ two-sided test revealed a\nsignificant difference between the parameters of the two groups.\nDiscussion\nFeigl and Zelen (1965) were able to\nshow in their work that they can predict survival time with different\nexponential distributions as a function of one or possibly several\nvariables. They were the first to perform a regression analysis of\nexponentially distributed survival times (Klein, Andersen, and Keiding 2014). This\nis a definite improvement over previous methods. However, there were\nsome aspects that simplified their model. First, they treated the\nvariable \\(wbc\\) as time independent,\nrecorded at the beginning of diagnostic analysis. For a more realistic\nmodel, this variable should be considered as time-dependent, as the\nnumber of white blood cells changes during the course of the disease.\nAnother aspect is the fact that only data from patients who have already\ndied were analysed. A year later, in the same journal, Zippin and Armitage (1966) applied and\nextended Feigl and Zelen’s approach to data that had been right\ncensored.\nThrough their analysis, Feigl and Zelen were able to identify three\nimportant use cases.\nThe comparison of survival curves incorporating different\nvariables and medical information.\nPredicting survival probabilities of individual patients based on\ndifferent variables.\nEvaluation of survival data from clinical trials.\nThis last point seems to be the most important. It is a step forward\nif more in-depth information on patients can be included in clinical\ntrials. In this way, the effects of treatments can be seen much more\nclearly. At the end of their work, they also show how other formulations\nof a model could arise. The first variant is \\(E(t_i) = (a + bx_i)^{-1}\\). This has the\nadvantage that \\(a\\) can be considered\nindependent and in this case we only test the hypothesis that \\(b = 0\\). Another alternative is the\nformulation \\(E(t_i) = ae^{bx_i}\\).\nHere it is assumed that the logarithm of the average survival time can\nbe considered as a linear function of the variable \\(x\\). I have used this model to reproduce\nthe authors’ results.\nIn conclusion, the authors’ contribution has advanced the field of\nsurvival analysis. Nevertheless, it must be said that the exponential\ndistribution has found limited use in biomedical applications, as its\nlack of aging property is too restrictive for most problems. Here, for\nexample, the Weibull distribution has clear advantages because it not\nonly allows a constant hazard rate.\nReproducing the results\nof Feigl & Zelen\nI present here the code and calculations with which the results of\nFeigl and Zelen (1965) have been\nreproduced. It is the following alternative model:\n\\[\n\\begin{aligned}\nE(t_i) &= ae^{bx_i}  \\\\\n&= ae^{bd_i}  \\; \\; with \\; \\; d_i = x_i - \\overline{x}\n\\end{aligned}\n\\]\n\n\n# Required library\nlibrary(tidyverse)\nplotutils::set_custom_theme(base_size = 26)\n\n\n\nIn their paper, the authors work with a data sample of leukemia\npatients with information about their white blood cell count (\\(x\\)) and the survival time in weeks (\\(t\\)) from date of diagnosis. Here we have\n\\(n = 17\\) data points of group 1 and\ntheir respective survival times. The number is quite small, but it\nserves only as a demonstration.\n\n\nx <- c(2300, 750, 4300, 2600, 6000, 10500, 10000, 17000, 5400, 7000, 9400,\n       32000, 35000, 100000, 100000, 52000, 100000)\nt <- c(65, 165, 100, 134, 16, 108, 121, 4, 39, 143, 56, 26, 22, 1,1,5,65)\n\nd <- x - mean(x)\n\n\n\nTo obtain parameter estimates the maximum likelihood method is\napplied. This method maximizes the joint probability density function\nwith respect to a certain distribution. It then computes the estimate of\nthe population parameter value that is the optimal fit to the observed\ndata. The maximum likelihood here can be written as follows:\n\\[\nL(a, b) = -n \\; log \\; a - \\sum_i \\frac{t_ie^{-bd_i}}{a}\n\\]\nNow the parameters have to be estimated. First we solve for an\nestimate for \\(\\hat b\\). Afterwards we\ncan use the result to receive an estimate for \\(\\hat a\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial b} &= 0 = \\sum_{i = 1}^n t_id_ie^{- \\hat\nbd_i} \\\\\n\\hat a &= \\sum_{i = 1}^n \\frac{t_ie^{-\\hat bd_i}}{n}\n\\end{aligned}\n\\]\nFor the calculation of \\(\\hat b\\),\nwe’ll use the Newton Raphson method. It is a root-finding algorithm that\ncan be applied iteratively. The idea is to start with an initial guess,\nthen to approximate the function by its tangent line, and finally to\ncompute the x-intercept of this tangent line. \\(\\hat{b}_k\\) is the estimate at the \\(k\\)th iteration.\n\\[\n\\hat{b}_{k + 1} = \\hat{b}_k - \\phi(\\hat{b}_k) / \\phi'(\\hat{b}_k)\n\\]\nIn our case the we are interested in the function \\(\\phi(\\hat{b})\\) as well as the first\nderivative \\(\\phi'(\\hat{b})\\).\n\\[\n\\begin{aligned}\n\\phi(\\hat{b}) &= \\sum_{i = 1}^n t_id_ie^{-\\hat{b}d_i} \\\\\n\\phi'(\\hat{b}) &= - \\sum_{i = 1}^n t_id_i^2 e^{-\\hat b d_i}\n\\end{aligned}\n\\]\n\n\n# We'll calculate the paramters with the Newton-Raphson method\n# Phi(b)\nf <- \\(b) {sum(t*d*exp(-b*d))}\n\n# The first derivative of Phi(b)\ndf <- \\(b) {-sum(t*d^2*exp(-b*d))}\n\n\n\nA first estimate of \\(\\hat b\\) is\nobtained as follows.\n\\[\n\\hat b_0 = \\sum_{i = 1}^n d_i \\;log\\; t_i \\; / \\;\\sum_{i = 1}^n d_i^2\n\\]\n\n\n# Initial value for b\nb <- sum(d*log(t))/sum(d^2)\neps <- 1\n\n# Iterative Newton-Raphson method\nwhile(eps > 1e-6){\n  b_new <- b - f(b)/df(b)\n  b <- b_new\n  eps <- abs(f(b)/df(b))\n}\n\n\n\nAfter that, we can then calculate \\(\\hat\na\\).\n\n\n# Calculate the estimate for a\na <- sum(t*exp(-b*d)/length(t))\n\n\n\nThe survival function is then as already described above as\nfollows:\n\\[\nS(t) = exp(-t \\; (ae^{bd})^{-1})\n\\]\n\n\n# Survival function\nsurvie <- \\(x) exp(-t*(1/(a*exp(b*x))))\n\n\n\nWe are looking at different values for the white blood cell count\nvariable so that we can compare survival probability curves.\n\n\nx_values <- c(2500, 5000, 10000, 20000, 35000, 50000, 75000, 100000)\nd_values <- x_values - mean(x_values)\n\n\n\nNow we apply our survie function and plot the result.\n\n\ns_df_1 <- tibble(s = unlist(map(d_values, survie)),\n               wbc = rep(x_values, each = length(x)),\n               t = rep(t, length(d_values))) |>  \n  bind_rows(tibble(s = rep(1, length(x_values)), # Starting values: \n                   t = rep(0, length(x_values)), # t = 0 -> s = 1\n                   wbc = x_values)) |> \n  mutate(ag = \"AG: positive\")\n\n# Create the plot\nggplot(s_df_1, aes(x = t, y = s, colour = as.factor(wbc))) +\n  geom_point(size = 2.7) +\n  geom_line(size = 1.9) +\n  labs(title = \"Modeling Survival Probability\",\n       x = \"Time (weeks)\",\n       y = \"Probability\",\n       colour = \"Different values of WBC\") +\n  scale_color_brewer(palette = \"YlOrRd\")\n\n\n\n\nHere we can directly see the effect of the white blood cell count\nvariable on the survival probablity curve for group 1 (AG: positive). To\nmake it easier to work with now we incorporate these parts into one\nfunction.\n\n\n# Put it in one function\nsurvie_fun <- function(x, t) {\n  \n  d <- x - mean(x)\n  \n  # Phi(b)\n  f <- \\(b) {sum(t*d*exp(-b*d))}\n  \n  # The first derivative of Phi(b)\n  df <- \\(b) {-sum(t*d^2*exp(-b*d))}\n  \n  # Inital valeur pour b\n  b <- sum(d*log(t))/sum(d^2)\n  eps <- 1\n  \n  # Methode de Newton-Raphson\n  while(eps > 1e-6){\n    b_new <- b - f(b)/df(b)\n    b <- b_new\n    eps <- abs(f(b)/df(b))\n  }\n  \n  # Calculer l'estimateur de a\n  a <- sum(t*exp(-b*d)/length(t))\n  \n  # On regarde les valeurs differents pour la covariate wbc (white blood count)\n  x_values <- c(2500, 5000, 10000, 20000, 35000, 50000, 75000, 100000)\n  d_values <- x_values - mean(x_values)\n  \n  # Calculer la survie pour les valeurs differents\n  survie <- \\(x) {exp(-t*(1/(a*exp(b*x))))}\n  \n  survie_df <- tibble(s = unlist(map(d_values, survie)),\n                   wbc = rep(x_values, each = length(x)),\n                   t = rep(t, length(d_values))) |>  \n    bind_rows(tibble(s = rep(1, length(x_values)),\n                     t = rep(0, length(x_values)),\n                     wbc = x_values))\n  return(survie_df)\n}\n\n# The values for Group 2\nx_2 <- c(4400, 3000, 4000, 1500, 9000, 5300, 10000, 19000, 27000, 28000, 31000,\n       26000, 21000, 79000, 100000, 100000)\nt_2 <- c(56, 65, 17, 7, 16, 22, 3, 4, 2, 3, 8, 4, 3, 30, 4, 43)\n\ns_df_2 <- survie_fun(x = x_2, t = t_2) |> \n  mutate(ag = \"AG: negative\")\n\ns_df <- bind_rows(s_df_1, s_df_2)\n\n# Plot the comparison\nggplot(s_df, aes(x = t, y = s, colour = as.factor(wbc))) +\n  geom_point(size = 1.7) +\n  geom_line(size = 0.9) +\n  facet_wrap(~ ag, scales = \"free_x\") +\n  labs(title = \"Modeling the Survival Probability\",\n       subtitle = \"Influence of Auer Bodies\",\n       x = \"Time (weeks)\",\n       y = \"Probability\",\n       colour = \"Different values of WBC\") +\n  scale_color_brewer(palette = \"YlOrRd\")\n\n\n\n\nWe managed to show that, also with this alternative model, for the\ngroup with the presence of Auer bodies (AG: positive) the survival\nprobability depends very strongly on the variable \\(wbc\\). This however is not the case when\nlooking at the group that do not show Auer bodies (AG: negative).\n\n\n\nFeigl, Polly, and Marvin Zelen. 1965. “Estimation of Exponential\nSurvival Probabilities with Concomitant Information.”\nBiometrics 21 (4): 826. https://doi.org/10.2307/2528247.\n\n\nKlein, John P., Per Kragh Andersen, and Niels Keiding. 2014.\n“Exponential Distribution as a Survival Model.” John Wiley\n& Sons, Ltd. https://doi.org/10.1002/9781118445112.stat06019.\n\n\nKleinbaum, David G., and Mitchel Klein. 2012. Survival\nAnalysis: A\nSelf-Learning Text.\nStatistics for Biology and Health. New York,\nNY: Springer New York. https://doi.org/10.1007/978-1-4419-6646-9.\n\n\nSeymour, J. F., and E. H. Estey. 1993. “The Prognostic\nSignificance of Auer Rods in Myelodysplasia.” British Journal\nof Haematology 85 (1): 67–76. https://doi.org/10.1111/j.1365-2141.1993.tb08647.x.\n\n\nZippin, Calvin, and Peter Armitage. 1966. “Use of Concomitant\nVariables and Incomplete Survival Information in the Estimation of an\nExponential Survival Parameter.” Biometrics 22 (4): 665.\nhttps://doi.org/10.2307/2528067.\n\n\n\n\n",
    "preview": "posts/2022-06-25-surviving-from-scratch/img/survival_img.jpg",
    "last_modified": "2023-03-05T12:22:02+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-20-not-so-standard-evaluations/",
    "title": "Not so standard evaluations",
    "description": "Getting a better understanding of the tidyeval framework",
    "author": [
      {
        "name": "Lukas Gröninger",
        "url": {}
      }
    ],
    "date": "2022-04-20",
    "categories": [
      "R",
      "NSE",
      "Metaprogramming"
    ],
    "contents": "\n\nContents\nList of helpful Ressources\nAn example of using NSE\nConclusion\n\nThe R language has some peculiarities. One of these is the use of non-standard evaluation (NSE) and the notion of Metaprogramming. I’m using R for quite some time now and these concepts have already given me some serious headaches (and will probably continue to do so…). Nevertheless, I have become a friend of this concept and enjoy using it. In the first two years of using R (it was my first programming language), I have not given a single thought to this idea, because I didn’t need it at all. Furthermore, I didn’t even question why the following was working:\n\n\nlibrary(dplyr)\n\nmtcars |> filter(cyl == 6)\n\n\n                mpg cyl disp  hp drat   wt qsec vs am gear carb\nMazda RX4      21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\nMazda RX4 Wag  21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\nHornet 4 Drive 21.4   6  258 110 3.08 3.21 19.4  1  0    3    1\nValiant        18.1   6  225 105 2.76 3.46 20.2  1  0    3    1\nMerc 280       19.2   6  168 123 3.92 3.44 18.3  1  0    4    4\nMerc 280C      17.8   6  168 123 3.92 3.44 18.9  1  0    4    4\nFerrari Dino   19.7   6  145 175 3.62 2.77 15.5  0  1    5    6\n\nHow does R know where to find cyl and why does both library(dplyr) and library(\"dplyr\") work?\nI don’t recall which problem I tried to fix, but one day I landed on a stackoverflow thread where one solution used the !! (bang-bang) operator. I have never seen it before and was puzzled. I copied the code and was glad that it worked, but I had no idea why. The more I started to define my own functions, the more I stumbled across this strange topic of non standard evaluations and Metaprogramming. There were more than a handful of expressions (😉) I couldn’t grasp. Examples are:\nQuasiquotation\n!! (Bang-Bang Operator)\n... (Dynamic dots)\nData Masking\n!!! (Big Bang or unquote-splice Operator)\nDefusing an expression\n{{}} (Embrace, Curly-Curly or Doublestashe Operator)\nClosures\nQuosures\nAll of these concepts seem highly complex and difficult and this post is not about trying to explain all or any of them in detail. For an R Beginner I feel these topics are quite intimidating and for me they would have been overwhelming and deterring. But have no fear, there are great ressources out there in the {rlang} documentation as well as in the Big Picture Chapture of Advanced R. What also helped me a lot was the Tidy Evaluation book by Lionel Henry and Hadley Wickham. In particular, the difference and the use cases between the Bang-Bang (!!) and the Big-Bang (!!!) operator became much clearer after reading it.\nSo what is this post about then? It should serve as a ressource (for myself and potentially others) to find helpful articles/posts or videos on the topic and to provide an example of how I use non standard evaluation in exploratory data analysis.\n\nI remember being really impressed after having seen how David Robinson used custom functions for interactive data exploration.\nList of helpful Ressources\nBlog Posts\nProgramming with Dplyr article\nTips on non-standard evaluation by Kun Ren\nStandard and non standard evaluation and On quosures by Brodie Gaslam\nTidy Eval is confusing sometimes by Kelly Bodwin\nVideos\nTidy Evaluation in 5 mins by Hadley Wickham\nUseful Tidy Evaluation in R by Connor Krenzer\nLazy Evaluation by Jenny Bryan\nR Metaprogramming Tutorial Playlist in german and english by Jurijy Weinblat\nOther\nquo vs enquo on stackoverflow\nTidy Eval use cases on R-Studio Community\nInteractivity and programming in the tidyverse by Lionel Henry\nAn example of using NSE\nI mainly use the Tidy eval framework in the realm of interactive data exploration and visualization. Here I will show how to apply it to create count summaries for multiple variables of a dataframe. I’ll use the tabyl() function from the {janitor} package. Often I just want to have a short look at all distinct values of a variable and the corresponding percentages. This is exactly what tabyl allows us to do. We start by loading the required libraries.\n\n\n# Required libraries\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(palmerpenguins) # Penguins Dataset to explore\n\n\n\nThe dataset for which we want to create count summaries is the penguins dataset from the {palmerpenguins} package. This is a classic dataset to play around with and offers eight variables.\nSelecting variables from penguins dataframeI really like this preview behaviour when accessing named lists or dataframes. What I also find enormously useful is the tidyselect syntax. Often the structure of variables in a dataframe follows a certain system. In this case we have variables that show directly in which metric they are measured (e.g. bill_length_mm). Tidyselect helpers offer a way to utilize this structure when selecting variables. There are several possibilities:1\n\n\n# Selecting all variables measured in mm\nselect(penguins, ends_with(\"_mm\"))\n\n# Selecting all variables containing the word \"bill\"\nselect(penguins, contains(\"bill\"))\n\n# Selecting all variables from island up to sex\nselect(penguins, island:sex)\n\n# Selecting specific variables\nselect(penguins, c(island, sex, year))\n\n\n\nLet’s have a look at the output when using tabyl. We want to look at the gender or sex values of penguins in our dataset.\n\n\njanitor::tabyl(penguins, sex)\n\n\n    sex   n percent valid_percent\n female 165   0.480         0.495\n   male 168   0.488         0.505\n   <NA>  11   0.032            NA\n\nHere we see that we get the count of distinct values and (valid) percentages. I want to store information like this for multiple variables in a named summary list. So how do I approach this problem?\nAs Jenny Bryan said in her Lazy evaluation talk you probably don’t have to go too deep in the rlang tidy evaluation framework. In many cases you can “just pass the dots”. This refers to the ... (Dynamic dots or forwarding) operator where you can collect multiple arguments to e.g. select() or group_by() calls.\n\n\n# Creating my custom var_summary function with `...`\nvar_summary <- function(df, ...) {\n  # Extract the column names in a character vector\n  col_names <- df |>\n    # Now I use the tidyselect helpers of the second argument\n    select(...) |>\n    names()\n  \n  # Map over the character column names and create the count summary\n  # In the end we want to name our so created list\n  col_names |>\n    map(~janitor::tabyl(df, !!sym(.x)) |> \n          arrange(desc(n)) |> \n          mutate(cum_percent = cumsum(percent))) |>\n    set_names(col_names)\n}\n\n# Test the function\nvar_sum <- var_summary(penguins, ends_with(\"_mm\"))\n\n\n\nAccessing the resulting var_sum listBesides the ... argument the !!sym(.x) stands out as a hint at a NSE pattern. What the sym() function does in our case is converting the character column name into a symbol that can be used as the second argument to the tabyl() function. This has to be evaluated through the use of the Bang-Bang operator. Let’s look at an example:\n\n\n# Define a character column name of our dataframe\ncol_name <- \"island\"\n\n# Convert to symbol\nsym_col_name <- sym(col_name)\n\n# Force evaluation of symbol in the context of the tabyl function\nvar_sum <- janitor::tabyl(penguins, !!sym_col_name) |> \n  arrange(desc(n)) |> \n  mutate(cum_percent = cumsum(percent)) \n\nvar_sum\n\n\n    island   n percent cum_percent\n    Biscoe 168   0.488       0.488\n     Dream 124   0.360       0.849\n Torgersen  52   0.151       1.000\n\n\nGreat, but there is also another possibility we can use as a second argument to our created function. The {{}} doublestache operator (term coined by Kelly Bodwin) which was introduced in rlang 0.4.0 (2019) can be used exactly for our use case as we only want to pass one argument of variable selection. The dynamic dots are used as a more general placeholder for multiple arguments. Here we just exchange the ... with the {{}}.\n\n\n# Creating my custom var_summary function with the doublestache operator\nvar_summary <- function(df, vars) {\n  # Extract the column names in a character vector\n  col_names <- df |>\n    # Now I use the tidyselect helpers of the second argument\n    select({{vars}}) |>\n    names()\n  \n  # Map over the character column names and create the count summary\n  # In the end we want to name our so created list\n  col_names |>\n    map(~janitor::tabyl(df, !!sym(.x)) |> \n          arrange(desc(n)) |> \n          mutate(cum_percent = cumsum(percent))) |>\n    set_names(col_names)\n}\n\n# Test the function\nvar_sum <- var_summary(penguins, c(species, island, sex))\n\n\n\nWhen reading about the tidyselect helper functions I thought it is probably possible to just extract the character vector of column names without the overhead of select-ing all the variables. And exactly for this purpose tidyselect::eval_select() exists. It evaluates a tidyselect expression like for example starts_with(\"bill\") in the context of a dataframe and returns a named integer vector with column names and position:\n\n\n# Capture an expression\nexpression <- expr(starts_with(\"bill\"))\n\n# Evaluating the selection in the dataframe environment\ntidyselect::eval_select(expression, data = penguins)\n\n\nbill_length_mm  bill_depth_mm \n             3              4 \n\nThe use of tidyselect::eval_select() leads to a slightly faster implementation of the function.\n\n\n# Creating my custom var_summary function using eval_select()\nvar_summary <- function(df, vars) {\n  expression <- rlang::enquo(vars)\n  col_names <- tidyselect::eval_select(expression, data = df) |>\n    names()\n  col_names |>\n    map(~janitor::tabyl(df, !!sym(.x)) |> \n          arrange(desc(n)) |> \n          mutate(cum_percent = cumsum(percent))) |>\n    set_names(col_names)\n}\n\nvar_sum <- var_summary(penguins, starts_with(\"bill\"))\n\n\n\nConclusion\nIn the end I created three different functions which did the same job highlighting different possibilities of how to use tidyeval/NSE patterns.\n\nI’ve written this post so that it provides a place of reference for me to come back to. Furthermore, it might lead to some people having a closer look at this paradigm and test their own ways of implementing Metaprogramming aspects in their workflow (I’ve just touched the surface here). I for myself find it a fascinating aspect of the R programming language which fits nicely into functional programming patterns. At the same time I have to admit, that it is hard to get your head around these ideas and see myself returning to the {rlang} documentation quite often…\n\nHere is a list with further examples↩︎\n",
    "preview": "posts/2022-04-20-not-so-standard-evaluations/img/glass.jpg",
    "last_modified": "2023-03-05T12:22:02+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-13-tidy-comparisons/",
    "title": "Tidy Comparisons",
    "description": "How to recode largish data in the tidy(table)verse.",
    "author": [
      {
        "name": "Lukas Gröninger",
        "url": {}
      }
    ],
    "date": "2022-02-13",
    "categories": [
      "R",
      "Benchmark"
    ],
    "contents": "\n\nContents\nThe {tidytable} package\nThe Scenario\nApproach 1: left_join()\nApproach 2: case_when()\nSpeed comparison\nConclusion\n\nA common task when when working with data, is the recoding of values. There are many reasons you might want to do that. Maybe you want to create different groups out of one variable or just translate some responses from german to english. In this blogpost I want to demonstrate how this can be achieved in two different ways. Additionally, I will have a look at the {tidytable} package developed by Mark Fairbanks.\nI feel quite at home in the tidyverse and had never really felt the necessity to deal with {datatable}. When I came across a tweet discussing {tidytable} I wanted to have a look. The reason why I don’t use {dtplyr} is that {tidytable} to my knowledge has a better support for tidy evaluation.\n\nAnother inspiration for this post was this discussion on R-Studio Community on how to quickly recode largish vectors in a tibble.\nThe {tidytable} package\nTo cite the first lines of the documentation page:\nWhy {tidytable}?\ntidyverse-like syntax built on top of the fast {data.table} package\nCompatibility with the tidy evaluation framework\nIncludes functions that dtplyr is missing, including many {tidyr} functions\nIf you want to learn more about the package, Bruno Rodriguez has written a great blogpost introducing it. The idea is that {tidytable} functions behave more or less like classic {dplyr} or {tidyr} functions to make it as easy as possible coming from the classic tidyverse. A dot at the end of the function indicates the {tidytable} version (e.g. mutate.() and not mutate()). So let’s start and load our necessary libraries:\n\n\n# Required libraries\nlibrary(tidyverse)\nlibrary(tidytable)\nlibrary(microbenchmark) \n\n# Set custom theme for plots\nplotutils::set_custom_theme(base_size = 34)\n\n\n\nThe Scenario\nWe’ve got several dataframes (tibble or tidytable) consisting only of one variable. For simplicity this variable is just composed of the 26 letters of the alphabet. Now I want to recode some of these letters. Like the letters, fruits are chosen here as an example completely arbitrarily. What we want to do is compare the speed of recoding some of these letters when the size of the dataframe increases. This means each dataframe has n rows and we test our different approaches with an increasing size of n. I decided to start with one thousand (1e3) rows and increase the size up to ten million (1e7) rows.\n\n\nn_values <- c(1e3, 1e4, 1e5, 1e6, 1e7)\n\n\n\nWhen operating on datasets with a million rows and more speed and efficient computation start to become an issue. I created the lists of dataframes by mapping over the n_values and filling the rows with letters from the alphabet.\n\n\nset.seed(1102)\n\ntibble_list <- map(n_values, function(.x) {\n  tibble(var = sample(letters, size = .x, replace = TRUE))\n})\n\ntidytable_list <- map(n_values, function(.x) {\n  tidytable(var = sample(letters, size = .x, replace = TRUE))\n})\n\n\n\nApproach 1: left_join()\nThe first approach I will test is using the left_join() function from the {dplyr} package. It is basically a dictionary or look-up table to recode the values of interest. That’s why we first create the table with the information which values should be recoded to what. In this made up case an “a” and a “c” should be recoded to “apple”, the “b”, “f” and “j” to “banana” and so on. As we only want to recode some letters, we fill up the dictionary with the other distinct values in our data. Otherwise we would get an NA value for every letter that does not need to be recoded.\n\nTo state the obvious _tib stands for tibble or classic tidyverse and _tidyt stands for tidytable.\n\n\n# Create the two dictionaries\ndict_tib <- tibble(key = c(\"a\", \"b\", \"c\", \"d\", \"f\", \"j\", \"m\", \"p\", \"u\", \"y\"),\n               value = c(\"apple\", \"banana\", \"apple\", \"mango\", \"banana\", \"banana\",\n                       \"mango\", \"papaya\", \"pear\", \"cherry\")) |> \n  full_join(tibble(key = letters)) |> \n  mutate(value = coalesce(value, key))\n\ndict_tidyt <- tidytable(key = c(\"a\", \"b\", \"c\", \"d\", \"f\", \"j\", \"m\", \"p\", \"u\", \"y\"),\n               value = c(\"apple\", \"banana\", \"apple\", \"mango\", \"banana\", \"banana\",\n                       \"mango\", \"papaya\", \"pear\", \"cherry\")) |> \n  full_join.(tibble(key = letters)) |> \n  mutate.(value = coalesce.(value, key))\n\n\n\nJust to be consistent I use the with a dot appended {tidytable} functions. The resulting dictionary looks like this:\n\n\ndict_tidyt |> \n  paged_table()\n\n\n\n\n\nNow I can define the functions to later compare in the benchmark test.\n\n\nleft_join_tib <- function(df) {\n  df |> \n    left_join(dict_tib, by = c(\"var\" = \"key\")) \n}\n\nleft_join_tidyt <- function(df) {\n  df |> \n    left_join.(dict_tidyt, by = c(\"var\" = \"key\")) \n}\n\n\n\nApproach 2: case_when()\nThe second approach is what I’m most familiar with and my goto solution when recoding values. It uses the case_when() function inside a mutate call. Here I can easily declare what values should be recoded to.\n\n\nrecode_vals_tib <- function(x) {\n  return(case_when(x %in% c(\"a\", \"c\") ~ \"apple\", \n                   x %in% c(\"b\", \"f\", \"j\") ~ \"banana\",\n                   x %in% c(\"d\", \"m\") ~ \"mango\",\n                   x == \"y\" ~ \"cherry\",\n                   x == \"p\" ~ \"papaya\",\n                   x == \"u\" ~ \"pear\",\n                   TRUE ~ x))\n}\n\nrecode_vals_tidyt <- function(x) {\n  return(case_when.(x %in% c(\"a\", \"c\") ~ \"apple\", \n                    x %in% c(\"b\", \"f\", \"j\") ~ \"banana\",\n                    x %in% c(\"d\", \"m\") ~ \"mango\",\n                    x == \"y\" ~ \"cherry\",\n                    x == \"p\" ~ \"papaya\",\n                    x == \"u\" ~ \"pear\",\n                    TRUE ~ x))\n}\n\n\n\nAs with the second approach I create the function with a dataframe as an input to compare in the benchmark test. The small dot at the end of the functions indicates the subtle difference.\n\n\ncase_when_tib <- function(df) {\n  df |> \n    mutate(var = recode_vals_tib(var))\n}\n\ncase_when_tidyt <- function(df) {\n  df |> \n    mutate.(var = recode_vals_tidyt(var))\n}\n\n\n\nSpeed comparison\nSource: www.pexels.comNow we will use the {microbenchmark} package to compare the difference in speed when recoding values depending on the selected approach.\n\n\nresult <- map2_df(.x = tibble_list, .y = tidytable_list,\n               .f = function(.x, .y) {\n                 microbenchmark(\n                   left_join_tib(.x),\n                   left_join_tidyt(.y),\n                   case_when_tib(.x),\n                   case_when_tidyt(.y),\n                   times = 50) |> \n                 group_by(expr) |> \n                 summarize(median_time = median(time, na.rm = TRUE))\n                 })\n\n\n\nI use the map2_df() function and give the two created lists of dataframes as an input. For each approach I test the classic tidyverse approach against the tidytable version. Afterwards I extract the median time it took. To compare the functions I plot them on a logarithmic scale on both axes.\n\n\nresult |> \n  mutate(n_rows = rep(n_values, each = 4),\n         median_time = median_time*1e-9) |> \n  ggplot(aes(x = n_rows, y = median_time, colour = expr)) +\n  scale_x_continuous(trans = \"log10\", labels = scales::label_number(big.mark = \",\")) +\n  scale_y_log10() +\n  geom_point(size = 2) +\n  geom_line(size = 1.4) +\n  scale_colour_viridis_d() +\n  labs(title = \"Speed Comparison\", \n       subtitle = \"X and Y Axis log transformed\",\n       colour = NULL,\n       x = \"Number of rows\",\n       y = \"Seconds\")\n\n\n\n\nWe can easily see that the {tidytable} functions outperform the classic tidyverse functions. The difference becomes more prominent when we keep the y-axis untransformed.\n\n\nresult |> \n  mutate(n_rows = rep(n_values, each = 4),\n         median_time = median_time*1e-9) |> \n  ggplot(aes(x = n_rows, y = median_time, colour = expr)) +\n  scale_x_continuous(trans = \"log10\", labels = scales::label_number(big.mark = \",\")) +\n  geom_point(size = 2) +\n  geom_line(size = 1.4) +\n  scale_colour_viridis_d() +\n  labs(title = \"Speed Comparison\", \n       subtitle = \"X Axis log transformed\",\n       colour = NULL,\n       x = \"Number of rows\",\n       y = \"Seconds\")\n\n\n\n\nConclusion\nFor the case of 10 million rows the dplyr::case_when() approach took around 3 seconds whereas the fastest approach was the tidytable::left_join() approach with \\(0.46\\) seconds (more than 6 times the difference). When compared directly the dplyr::left_join() took at 1 million rows \\(45.5\\%\\) and at 10 million rows \\(31.5\\%\\) longer than the {tidytable} alternative.\nIn general the results show that when operating with dataframes of around 10 thousand rows the differences are negligible. Only when the data you are dealing with becomes significantly larger (more than 1 million rows) it is more reasonable to use the {tidytable} alternative. Furthermore, the left_join() variant is probably a bit cleaner or better to manage as you store the dictionary in a separate table. The great thing about {tidytable} is that the switch is so easy.\n\n\n\n",
    "preview": "posts/2022-02-13-tidy-comparisons/img/tidy_table.jpg",
    "last_modified": "2023-03-05T12:22:02+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-01-keep-up-your-standards/",
    "title": "Keep up your Standards",
    "description": "How to set up default files to nudge you in the right direction.",
    "author": [
      {
        "name": "Lukas Gröninger",
        "url": {}
      }
    ],
    "date": "2022-02-01",
    "categories": [
      "R",
      "R-Studio"
    ],
    "contents": "\n\nContents\nIntroduction\nWhat are good standards?\nCreating default R templates\nThe next step\nConclusion\n\nIntroduction\nThis post is about setting up new standard R and R-Markdown files to help you benefit from the default effect. This effect makes use of the fact that people are inherently lazy. We suffer from the status quo bias. That means we tend to leave things as they are. This is why we just agree to General Terms and Conditions and accept all cookie tracking on websites. Companies and organizations are well aware of this flaw of ours and take advantage of it. This means we can leverage this behaviour as well to e.g. improve our code documentation. It is good practice to design decision architectures to “Improving decisions about health, wealth, and happiness” (Thaler and Sunstein 2009).\n\nYou will find lots of literature on the topic in the field of behavioural economics. Prominent authors are Daniel Kahnemann (Thinking, fast and slow) and Richard Thaler (Nudge)\nWhat are good standards?\nCode documentation is crucial for:\nMaintainability\nReproducibility\nGeneral knowledge transfer\n\nWhen we create a new script we often don’t bother to give it a title, date or a short description about it’s purpose and further information. After all, we know exactly what we need it for, right? The problem is that this is not the case in 4 or 5 months from now. And even less if someone else wants to understand why this script exists. But we can set ourselves up for success by using smart default templates.\nDisclaimer:\nI will demonstrate how to change the default templates on a Windows machine using RStudio. The process for Macs and another IDE will look different.\nCreating default R templates\nCurrently when we go to New File -> R Script or type Ctrl+Shift+N we see an empty script. To change that we have to create a templates folder first. In this new folder we’ll create a new default.R file. Now we can edit this file as we like.\n\n\n# Required libraries\nlibrary(fs)\nlibrary(usethis)\n\n# Create a templates folder\ndir_create(path = \"~/AppData/Roaming/RStudio/templates\")\n\n# Create the file, here we start with a normal R script\nfile_create(\"~/AppData/Roaming/RStudio/templates/default.R\")\n\n# Open the file in RStudio to edit it\nedit_file(\"~/AppData/Roaming/RStudio/templates/default.R\")\n\n\n\nIt is helpful to specify some meta information like Author, E-Mail or Date especially when these scripts are being shared with others. Another aspect is to use the code folding feature (shown in line 11).\nThe result might look like this:\nModify default.RWe can do now the same for R-Markdown files. The procedure is the same.\n\n\n# Now we do the same with an RMarkdown Script\nfile_create(\"~/AppData/Roaming/RStudio/templates/default.Rmd\")\n\n# And open the file in RStudio again\nedit_file(\"~/AppData/Roaming/RStudio/templates/default.Rmd\")\n\n\n\nHere the template is somewhat more verbose. Often you know what things you enter repeatedly. These templates may look different for everyone.\nModify default.RmdThe next step\nYou can not only create default templates for your local machine. The next step would be to create an R package for the sole purpose of containing a set of standardized default templates for e.g. your organization or your team. Let’s imagine you work in a company and you want to create specific reports on a regular basis. This would be a good use case to create a template for that.\nHere is a great resource for doing that. I used it to create the {TestTemplates} package. Now everyone can just install the package and be able to use your templates.\nTestTemplates PackageConclusion\nIn this post I went through the process of setting up default R templates. They do not ensure that all at once your code structure will be perfect. The idea is to reflect and work out how standards should be constructed for your particular case. The purpose of these templates is only to nudge us in the right direction towards a good code structure as well as programming practices.\n\n\n\nThaler, Richard H., and Cass R. Sunstein. 2009. Nudge: Improving Decisions about Health, Wealth, and Happiness. New York: Penguin Books.\n\n\n\n\n",
    "preview": "posts/2022-02-01-keep-up-your-standards/img/standard_1.jpg",
    "last_modified": "2023-03-05T12:22:02+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-10-scraping-used-boats/",
    "title": "Scraping used boats",
    "description": "How to find the perfect sailing yacht.",
    "author": [
      {
        "name": "Lukas Gröninger",
        "url": {}
      }
    ],
    "date": "2022-01-10",
    "categories": [
      "R",
      "Webscraping"
    ],
    "contents": "\n\nContents\nWeb Scraping\nData Cleaning\nFurther steps\nGeographical Representation\nConclusion\n\nIn the last post I described how to use an API to download financial data.\nIn this post I want to talk about accessing data from the web when we don’t have\nthe luxury of using an API. Maybe there is an interface, but it is not publicly\navailable. Or there is none and we still want to extract data.\nThe concept we will use here is called Web Scraping.\nIt is about collecting structured web data in an automated way.\nThe example to demonstrate this process will be the website\nwww.boat24.com.\n\nIt is a european two sided marketplace\nto purchace or sell used sailing or motorboats. I’ve worked on ships for some\nyears and enjoy going sailing during vacation. And so sometimes I end up looking at\nused sailboats on the Internet… Today the goal is to scrape information\nfrom this site.\nLet’s have a look:\nSource: www.boat24.com/en/sailboats/We filtered for sailboats and see a list of boats and additional information.\nThe total number of listings is 3296 boats. The general setup of the page\nis already quite good for our task.\nWeb Scraping\nNow I want to access information about the\ninventory of this marketplace. We can do this by investigating the\nstructure of the website of interest.\n\nMake sure that it’s okay to scrape the site by looking at their robots.txt file.\nThis means answering questions like -\nHow is the content and the information loaded and displayed and what other requests are made in the background?\nWe do this by having a look at the Developer Tools Panel (if you are using a Chrome Browser).\nOn Windows this opens up after typing Control+Shift+C.\nWe will use it to inspect the different elements and extract and parse the relevant html/xml.\nThe libraries we’ll need to perform the process of extracting information from\na web page are the following:\n\n\nlibrary(tidyverse)  # Main data wrangling\nlibrary(rvest)      # Web scraping library\nlibrary(xml2)       # XML Parsing \nlibrary(lubridate)  # Handling dates\nlibrary(sf)\n\n# Setting theme\nplotutils::set_custom_theme(base_size = 26)\n\n\n\nWe start by scrolling through the boats and click on a link in order to see a\nmore detailed view of the specific boat listing. We see pictures of the boat\nand further information in a table below. When hovering over this table we\nsee the section is called “specs”.\nSpecs TagWe are definitely interested in extracting this information.\nTo identify this table we can right click and copy the xpath.\nBut first we will set up our R script. We see that on every page there are\n20 boats listed. The starting url from where we’ll navigate this site will\ntherefore be https://www.boat24.com/en/sailboats/?page=0&sort=datdesc.\n\n\n# Starting url\nurl <- \"https://www.boat24.com/en/sailboats/?page=0&sort=datdesc\"\n\n# Starting page number\npage_num <- 0\n\n\n\nAfterwards we’ll initialize our dataframe with all the variables of interest\nthat we want to extract.\n\n\n# Initialize Dataframe (for the moment only char-variables, will be cleaned later)\nboat_df <- tibble(id = character(),\n                  type = character(),\n                  price = character(),\n                  condition = character(),\n                  manufacturer = character(),\n                  model = character(),\n                  region = character(),\n                  location = character(),\n                  year = character(),\n                  length_width = character(),\n                  draft = character(),\n                  displacement = character(),\n                  material = character(),\n                  steering = character(),\n                  keel_type = character(),\n                  ballast = character(),\n                  no_persons = character(),\n                  headroom = character(),\n                  no_cabins = character(),\n                  no_beds = character(),\n                  propulsion = character(),\n                  engine_perf = character(),\n                  fuel_type = character(),\n                  fuel_cap = character(),\n                  engine_hours = character(),\n                  mainsail = character(),\n                  genoa = character(),\n                  spinnaker = character(),\n                  ad_date = character(),\n                  no_views = character(),\n                  no_favs = character())\n\n\n\nThe process looks like this:\nStart with the landing page and find the list of boat links.\nAccess each of these pages and extract relevant information.\nGo to the next page (increase the page_num by 20) and repeat.\nI will break it down into several steps.\n\n# Start while loop with page_num = Number of Boats displayed at the top left                \nwhile (page_num < 3280) {\n  \n  website <- read_html(url)\n  # Extract boatlist of website\n  boat_list <- website  |>  \n    html_nodes(xpath = \"/html/body/main/section[2]/div/ul/li\")\n\nWe’ll use the read_html() function from the {rvest} package to extract all\nof the html. with html_nodes() we can specify which segment we want to read.\nWe are interested in the links of the detail pages of the boats.\nWith the help of the developer tool view we can select the element of interest, right click and\ncopy the xpath. In the case of the specs table this was “//*[@id=‘specs’]“.\nFor the ID, the price or the number of views, there are different xpaths which\ncan be found by inspecting the elements of the site.\nIn order to extract the relevant information we need to use some regular\nexpressions (regex). To extract the condition of the boat we use for example a\nlookahead “[^\\n]+(?=Condition?)”.\n\n# Loop through the 20 boats on the list\n  for (i in seq_along(boat_list)) {\n    \n    boat_link <- xml_attrs(xml_child(boat_list[[i]], 1))[2]  \n    # There are some google ads which have to be filtered by this if statement\n    if (str_sub(boat_link, start = 1L, end = 22L) == \"https://www.boat24.com\") {\n      \n      boat_link_site <- read_html(boat_link)\n      # Read out the variables of interest with xpath or classes\n      specs_table <- html_nodes(boat_link_site, \n                                xpath = \"//*[@id='specs']\") %>% html_text()\n      \n      id <- html_nodes(\n        boat_link_site, \n        xpath = \"//*[@id='sticky-header-trigger']/div/div/aside/section[2]/ul[1]/li[1]/strong\")  |>  \n        html_text() \n      type <- html_nodes(\n        boat_link_site, \n        xpath = \"//*[@id='specs']/header/p[1]/a[1]\") |>  \n        html_text() \n      price <- html_nodes(\n        boat_link_site, \n        xpath = \"//*[@id='contact']/div[1]/p[1]/strong\")  |>  \n        html_text()  \n      region <- html_nodes(\n        boat_link_site, \n        xpath = \"//*[@id='location']/p/text()\") |>  \n        html_text() \n      location <- html_nodes(\n        boat_link_site, \n        xpath = \"//*[@id='location']/p/strong\") |>  \n        html_text() \n\n      condition <- str_extract(specs_table, \"[^\\n]+(?=Condition?)\")\n      manufacturer <- str_extract(specs_table, \"[^\\n]+(?=Manufacturer?)\")\n      model <- str_extract(specs_table, \"[^\\n]+(?=Model?)\")\n      year <- str_extract(specs_table, \"\\\\d+(?=Year Built?)\")\n      length_width <- str_extract(specs_table, \"[^\\n]+(?=Length x Width?)\")\n      draft <- str_extract(specs_table, \"[^\\n]+(?=Draft?)\")\n      displacement <- str_extract(specs_table, \"[^\\n]+(?=Displacement?)\")\n      material <- str_extract(specs_table, \"[^\\n]+(?=Material?)\") \n      steering <- str_extract(specs_table, \"[^\\n]+(?=Steering?)\")\n      keel_type <- str_extract(specs_table, \"[^\\n]+(?=Keel Type?)\")\n      ballast <- str_extract(specs_table, \"[^\\n]+(?=Ballast?)\")\n      no_persons <- str_extract(specs_table, \"[^\\n]+(?=Certified No. of Persons?)\")\n      headroom <- str_extract(specs_table, \"[^\\n]+(?=Headroom?)\")\n      no_cabins <- str_extract(specs_table, \"[^\\n]+(?=No. of Cabins?)\")\n      no_beds <- str_extract(specs_table, \"[^\\n]+(?=No. of Beds?)\")\n      propulsion <- str_extract(specs_table, \"[^\\n]+(?=Propulsion?)\")\n      engine_perf <- str_extract(specs_table, \"[^\\n]+(?=Engine Performance?)\")\n      fuel_type <- str_extract(specs_table, \"[^\\n]+(?=Fuel Type?)\")\n      fuel_cap <- str_extract(specs_table, \"[^\\n]+(?=Fuel Capacity?)\")\n      engine_hours <- str_extract(specs_table, \"[^\\n]+(?=Engine Hours?)\")\n      mainsail <- str_extract(specs_table, \"[^\\n]+(?=Mainsail?)\")\n      genoa <- str_extract(specs_table, \"[^\\n]+(?=Genoa?)\")\n      spinnaker <- str_extract(specs_table, \"[^\\n]+(?=Spinnaker?)\")\n      \n      ad_date <- html_nodes(\n        boat_link_site, \n        xpath = \"//*[@id='sticky-header-trigger']/div/div/aside/section[2]/ul[1]/li[2]/strong\") |>  \n        html_text() \n      no_views <- html_nodes(\n        boat_link_site, \n        xpath = \"//*[@id='sticky-header-trigger']/div/div/aside/section[2]/ul[1]/li[3]/strong\") |>  \n        html_text() \n      no_favs <- html_nodes(\n        boat_link_site, \n        xpath = \"//*[@id='sticky-header-trigger']/div/div/aside/section[2]/ul[1]/li[4]/strong\") |>  \n        html_text()\n\nAfter having stored all information in variables we fill them in a dataframe/tibble.\nFor some boats there were missing variables which had to be coded as NA.\nWe can write a short function to do that.\n\n\n# Function to select only the first element, if empty --> NA\ncheck_input <- function(x) if (is_empty(x)) NA else x[[1]]\n\n\n\n\n      extracted_vars <- list(id, type, price, condition, manufacturer, model, region,\n                             location, year, length_width, draft, displacement,\n                             material, steering, keel_type, ballast, no_persons,\n                             headroom, no_cabins, no_beds, propulsion, engine_perf,\n                             fuel_type, fuel_cap, engine_hours, mainsail, genoa,\n                             spinnaker, ad_date, no_views, no_favs)\n\n      df <- extracted_vars |> \n        set_names(colnames(boat_df)) |> \n        map_df(check_input) \n      \n    }\n    # Rowbind the dataframe \n    boat_df <- bind_rows(boat_df, df)\n  }\n\nAfterwards the dataframe is appended to our previously created one.\nThen we have to increase the page_num counter and go to the next page.\n\n# Jump to the next page in steps of 20 (boats per page)\n  page_num <- page_num + 20\n  url <- paste0(\"https://www.boat24.com/en/sailboats/?page=\", \n                as.character(page_num), \"&sort=datdesc\")\n}\n\nIn the end we can save our final dataframe as a csv or any other file format.\nNow we’ll look at the first entries of our data:\n\n\nboat_df |> \n  head(3) |> \n  paged_table()\n\n\n\n\n\nFor simplicity the variables are all encoded as character.\nThe next step is to clean the data.\nData Cleaning\nThe main function to clean the data is the readr::parse_number() function.\nWith across() we can apply this function to several variables.\nAnother aspect is the fact that the prices are not displayed in a consistent format.\nThat’s why I had to extract the currency and convert all prices in Euro.\n\n\ncleaned_df <- boat_df |>  \n  drop_na(id) |> \n  distinct(id, .keep_all = TRUE) |> \n  separate(col = engine_perf, into = c(\"no_eng\", \"hp_eng\"), sep = \"x\") |>  \n  separate(col = length_width, into = c(\"length\", \"width\"), sep = \"x\") |> \n  mutate(currency = str_sub(price, 1, 1),\n         price = parse_number(price, \n                              locale = locale(decimal_mark = \",\", \n                                              grouping_mark = \".\")),\n         hp_eng = str_extract(hp_eng, pattern = \"(\\\\d)+(?= HP)\"),\n         displacement = parse_number(displacement,\n                                     locale = locale(grouping_mark = \"'\")),\n         across(.cols = c(no_persons, draft, no_cabins, no_beds, headroom, fuel_cap, \n                engine_hours, no_views, no_favs, ballast, mainsail, genoa, year,\n                spinnaker, no_eng, hp_eng, length, width),\n                .fns = parse_number),\n         region = case_when(str_detect(region, \"United Kingdom\") ~ \"United_Kingdom\",\n                            str_detect(region, \"United States\") ~ \"USA\",\n                            TRUE ~ region),\n         country = str_extract(region, pattern = \"\\\\w+\"),\n         price = case_when(currency == \"£\" ~ price*1.18,\n                           currency == \"C\" ~ price*0.9,\n                           currency == \"D\" ~ price*0.13,\n                           currency == \"E\" ~ price*1,\n                           currency == \"S\" ~ price*0.1,\n                           currency == \"U\" ~ price*0.85,\n                           TRUE ~ NA_real_),\n         ad_date = dmy(ad_date))\n\n\n\nFurther steps\nNow we could do some analysis, picking our favorite boat etc. We could calculate\nstatistics of which boats get the most views or are getting marked as\nfavorites most often. We could try to answer questions like -\nDoes the country where I sell my boat has an impact on the price? and so on.\nGeographical Representation\nAt last I will give a geographical view on how many boats are listed in different\neuropean countries. Here I use a dataframe where I scraped information from not only sailing\nbut motorboats as well. To plot it on a map, there are packages like\nrnaturalearth or spData which offer datasets for spatial analysis.\nThere are lots of great tutorials\nwhich show how to do that. I work with simple Features (sf)\nobjects and {ggplot2}.\n\n\nggplot() +\n  geom_sf(data = df_e, aes(fill = n_listings)) +\n  coord_sf(xlim = c(-15, 45), ylim = c(32, 70), expand = FALSE) +\n  scale_fill_viridis_c(direction = -1) +\n  labs(fill = \"Number of listings\") +\n  guides(fill = guide_colorbar(title.position = \"top\", \n                                title.hjust = .5, \n                                barwidth = 10, \n                                barheight = 1))\n\n\n\n\nWe can directly see that this marketplace focuses mainly on the german/swiss market.\nThe portfolio of boats outside of Central/Western Europe is negligible.\nConclusion\nIn this post I wanted to show how to scrape information from a webpage.\nIt is a useful skill to be able to analyze the structure of websites\nand see how they are built. I went with the Boat24.com example as I am interested\nin sailing yachts but the use cases are quite diverse.\nThe Internet offers the largest source of data and with Webscraping it can be accessed.\nYou could use it to compare prices of products on different websites, monitor competitors or\njust for general research tasks.\n\n\n\n",
    "preview": "posts/2022-01-10-scraping-used-boats/img/used_boats.jpg",
    "last_modified": "2023-03-05T12:22:02+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-15-apis-and-parameterized-reports/",
    "title": "API's and parameterized reports",
    "description": "How to access the SimFin API for financial reporting.",
    "author": [
      {
        "name": "Lukas Gröninger",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [
      "R",
      "API",
      "reporting"
    ],
    "contents": "\n\nContents\nAPI’s\nSimFin\nAccessing an API\nThe {simfinapi} package\n\nReporting with R\nStock prices over the year\nCreating your own metric\n\nConclusion\n\nAPI’s\nAPI’s (Application Programming Interfaces) are crucial when it comes to applications or systems communicating with each other. Today they are everywhere around us and I guess we cannot imagine a world without them. We are interacting with API’s several times a day e.g. when we are using Google Maps or sending out a tweet.\nSource: www.pexels.comIn the past years many organizations have decided to use an API-first approach for their projects. I will not go into detail about what exactly an API is or how it works. There are many resources explaining it (here for example).\nIn this blogpost I want to first show how to access the SimFin API and therefore gain access to financial information of more than 3000 public companies. Second I will demonstrate how to create your own financial (or any other) report.\nSimFin\nSimFin stands for simplifying finance and is an organization that provides fundamental financial data about public companies for everyone. In order to get an API-key and download data you need to register with a mail account first. It is recommended to store the API-key in an environment variable. Here is an article that describes how to do that. After having done that you are good to go.\nAccessing an API\nFirst we’ll load all necessary libraries.\n\n\n# required libraries\nlibrary(tidyverse) \nlibrary(here)\nlibrary(simfinapi)\nlibrary(httr2)      # communicating with API's through R\nlibrary(lubridate)  # Handling dates\nlibrary(gt)         # Pretty tables\nlibrary(gtExtras)\n\n# Setting theme\nplotutils::set_custom_theme(base_size = 30)\n\n\n\nOn the simfin website you will find a link to their Web API documentation. We start with an easy example of communicating with the API where we’ll request a list of available companies and their SimFinId.\nFirst we’ll store our API-key in a variable and create our url (web-address) with which we want to talk to. This information is available in the documentation.\n\n\nmy_apikey <- Sys.getenv(\"SIMFIN_KEY\")\n\nbase_url <- \"https://simfin.com/api/v2/\"\n\nendpoint <- \"companies/list\"\n\n# Create url\nurl <- paste0(base_url, endpoint, \"?api-key=\", my_apikey)\n\n\n\nNow we could just copy paste this url in our browser and see the result:\nInserting our url in the browserBut fortunately there is a more elegant R package for communicating with API’s. The {httr2} package is an advancement of the {httr} package developed by Hadley Wickham.\n\n\n# create the request\nreq <- request(url) |> \n  req_perform()\n\n# Check if it worked\nresp_status(req)\n\n\n[1] 200\n\nGlad to see that the request worked. Now we want to parse it’s raw output. In our case this is done via the resp_bod_json() function (as we are dealing with json data - see the browser output). Then we are converting it to a dataframe/tibble.\n\n\ncontent_json <- resp_body_json(req) \n\ncontent_df <- tibble(simfin_id = map_dbl(content_json$data, 1),\n                     ticker = map_chr(content_json$data, 2))\n\n\n\nLet’s have a look at the first few rows of our dataframe of available companies:\n\n\nAvailable companies\n    simfin_id\n      ticker\n    854465\n1COV.DE45846\nA1253413\nA181205636\nA20367153\nAA939324\nAAC_delist68568\nAAL\n\nAfter scrolling through the first entries we recognize the AAPL ticker belonging to Apple. Now we’ll have a closer look at this company and extend our request with more fields.\n\n\n# Define ticker. We want to have a look at Apple\nticker <- \"AAPL\"\n\n# the statement to retrieve. (profit/loss)\nstatement <- \"pl\"\n\n# the period & financial year to retrieve\nperiod <- \"q4\"\nfyear <- 2021\n\n# create url\nurl <- paste0(base_url, \"companies/statements?api-key=\", my_apikey, \"&ticker=\", ticker, \n              \"&statement=\", statement, \"&period=\", period,\"&fyear=\", fyear)\n\n# make request\napple_req <- request(url) |> \n  req_perform()\n\n# convert JSON\napple_json <- resp_body_json(apple_req)[[1]]\n\napple_df <- tibble(variable = map_chr(apple_json$columns, 1),\n                   value = map_chr(apple_json$data[[1]], 1, .null = NA_character_))\n\n\n\nNow we can inspect some information for Apple’s last quarter.\n\n\nApple's 4. Quarter 2021\n    variable\n      value\n    Ticker\nAAPLReport Date\n2021-09-30Publish Date\n2021-10-29Revenue\n83360000000.000000Gross Profit\n35174000000.000000Operating Expenses\n-11388000000.000000Research & Development\n-5772000000.000000\n\nThe {simfinapi} package\nAccessing the API in this way is somewhat cumbersome for more complex matters. But thankfully someone has written an R package for this as well.\nThanks to Matthias Gomolka for maintaining the {simfinapi} R package. This package wraps the SimFin API and allows us an easier access.\nFirst we set our API-key and a cache directory for our requests.\n\n\n# Setting the api key\nsfa_set_api_key(api_key = Sys.getenv(\"SIMFIN_KEY\"))\n# Setting up cache directory\nsfa_set_cache_dir(here(\"_posts/2021-12-15-apis-and-parameterized-reports\", \n                       \"simfin_cache\"), create = TRUE)\n\n\n\nNow we can use a set of functions to retrieve the information of interest. Our first request can be rewritten in one line as this:\n\n\navailable_companies <- sfa_get_entities()\n\n\n\nIf we want to look at some basic information for a company like Apple, we would write:\n\n\ncompany_infos <- sfa_get_info(ticker = \"AAPL\")\n\n\n\nReporting with R\nOne of the biggest strengths of R is its ability to produce beautiful reproducible reports and articles. This is done with {RMarkdown} and additional packages like {knitr}, {distill} etc. To learn more, I recommend the RMarkdown Cookbook by Yihui Xie, Christophe Dervieux and Emily Riederer.\nFor our small example report we want to focus on a specific set of companies. Let’s say we want to know how the stock prices of different social media networks performed in 2020. Were all companies equally affected by the Corona Virus? What we want also is to be able to change the set of companies as well as the year of interest programmatically when rendering the report. RMarkdown lets you define specific parameters at the top that you can access then inside your R code chunks.\nTo include these parameters in our report we have to define them first in the YAML header of our document. In our case this may look like this:\n\nparams:\n  stocks: [\"FB\", \"TWTR\", \"SNAP\", \"PINS\"]\n  fiscal_year: 2020\n\nHere we provide a list of stocks and the fiscal year as variables. We can access for example the list of stocks in our script with params$stocks.\nStock prices over the year\nIn this manner we can now use our parameters as arguments for the {simfinapi} functions.\n\n\n# Get company infos\ncompany_infos <- sfa_get_info(ticker = params$stocks)\n\n# Get stock prices of companies of interest\nshares <- sfa_get_prices(ticker = params$stocks) |> \n  left_join(company_infos)\n\n\n\nNow we can generate an outpot comparing the stock price development over the year we specified before.\n\n\nshares |> \n  mutate(year = year(date)) |> \n  # Filter only prices for our year of interest\n  filter(year == params$fiscal_year) |> \n  ggplot(aes(x = date, y = adj_close)) +\n  geom_line() +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  facet_wrap(~company_name, scales = \"free_y\") +\n  scale_x_date(date_breaks = \"3 months\", date_labels = \"%b\") +\n  labs(title = glue::glue(\"Stock prices over the year {params$fiscal_year}\"),\n       x = NULL,\n       y = NULL)\n\n\n\n\nWe can not only filter by our parameters but also using it to dynamically change the title of the plot. With different scales it is hard to compare the development between the selected companies. That’s why we’ll produce a plot with a logarithmized y axis.\n\n\nshares |>  \n  mutate(year = year(date)) |>  \n  filter(year == params$fiscal_year) |> \n  ggplot(aes(x = date, y = adj_close, colour = company_name)) +\n  geom_line(lwd = 1) +\n  scale_y_continuous(trans = \"log10\", labels = scales::dollar_format()) +\n  viridis::scale_colour_viridis(discrete = TRUE) +\n  scale_x_date(date_breaks = \"3 months\", date_labels = \"%b\") +\n  labs(title = \"Comparison with logarithmized y axis\",\n       x = NULL,\n       y = NULL,\n       colour = NULL) \n\n\n\n\nHere we can see directly the comparison between companies and specifically the “Corona effect” in the middle of march. By now I guess many have seen logarithmized scales as they were quite popular to illustrate exponential growth of Corona Virus cases. The difference between 10$ and 100$ is the same as the difference between 30$ and 300$. The social network whose share price has risen the most is Pinterest. One hypothesis is that Pinterest in particular could benefit from the retreat into the own four walls. On this platform, the topics of designing and decorating the home, garden etc. are popular.\nCreating your own metric\nNow we don’t want to only compare the development of the stock prices, but have a deeper look at some characteristics of these companies. The SimFin API offers a variety of variables and information. We can access them through the {simfinapi} function sfa_get_statement(). For now, we want to calculate our own metric to compare our four companies. This metric will be the Rule of 401 which is typically applied to SaaS (Software as a Service) companies to evaluate their development. Two metrics will be combined here. One is the growth of the company and the other is profit. In the first years of a company, it is common that there is no profit yet, but an emphasis on growth. As a rule of thumb the sum of growth and profit should equal to at least 40. For example 50% Growth and -10% Profit Margin would still satisfy this condition.\n\\[\nRule\\; of\\; 40 = Growth + Profit\\; Margin\n\\]\nThis simple metric can, of course, also be weighted if one of the two components is considered more relevant. Now let’s examine what this metric looks like for companies at the end of the fiscal year.\n\n\n# The net profit margin information is found in the \"derived\" statement\nderived_q4 <- sfa_get_statement(ticker = params$stocks,\n                                fyear = params$fiscal_year,\n                                period = \"q4\",\n                                statement = \"derived\")\n\nderived_q3 <- sfa_get_statement(ticker = params$stocks,\n                                fyear = params$fiscal_year,\n                                period = \"q3\",\n                                statement = \"derived\")\n\n# The revenue information can be found in the pl (profit/loss) statement\nprofit_loss_q4 <- sfa_get_statement(ticker = params$stocks,\n                                    fyear = params$fiscal_year,\n                                    period = \"q4\",\n                                    statement = \"pl\")\n\nprofit_loss_q3 <- sfa_get_statement(ticker = params$stocks,\n                                    fyear = params$fiscal_year,\n                                    period = \"q3\",\n                                    statement = \"pl\")\n# Define our own Metric\nrule_40 <- profit_loss_q4 |> \n  select(ticker, fiscal_year, revenue) |> \n  mutate(growth = revenue/profit_loss_q3$revenue - 1) |> \n  inner_join(select(derived_q4, net_profit_margin, ticker)) |> \n  mutate(rule_40 = growth + net_profit_margin,\n         revenue = revenue/1e6)\n\n\n\n\n\nRule of 40\n    ticker\n      fiscal_year\n      revenue\n      growth\n      net_profit_margin\n      rule_40\n    FB\n2020\n28071\n0.307\n0.400\n0.707PINS\n2020\n706\n0.594\n0.295\n0.889SNAP\n2020\n911\n0.343\n-0.124\n0.219TWTR\n2020\n1289\n0.377\n0.172\n0.549\n\nPinterest reached with almost 60% the highest growth relative to the previous quarter. Facebook - or Meta as it’s now being called - had the highest profitability. Added together only Snap was not able to satisfy the Rule of 40 metric.\nIn such a way, reports can now be created programmatically. If we want to e.g. exchange Snapchat with Google (Ticker is GOOG), we would render our report as follows:\n\n\nrmarkdown::render(my_report.Rmd, \n                  params = list(stocks = c(\"FB\", \"TWTR\", \"GOOG\", \"PINS\")))\n\n\n\nConclusion\nThere are countless possibilities to create specific reporting templates and functions for your own use cases. Maybe you need to report regularly on companies or industries? This post should give you the tools to do that by accessing the SimFin API. A really minimal example of how such a report might look like can be found here in the Github repo of this blogpost. The output format is of course not limited to html, but can be PDF as well.\nAnother use case could be creating a report to regularly display Social Media statistics or the number of people who visited your website. For the latter purpose you could access the Google Analytics API e.g. via the {googleAnalyticsR} package. However, the setup would be similar to the one presented here.\n\nhttps://kpisense.com/glossary/rule-of-40↩︎\n",
    "preview": "posts/2021-12-15-apis-and-parameterized-reports/img/use_apis.jpg",
    "last_modified": "2023-03-05T12:22:02+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-05-looping-over-the-bridge/",
    "title": "Looping over the bridge",
    "description": "Comparing loops by simulating a Squid Game scene",
    "author": [
      {
        "name": "Lukas Gröninger",
        "url": {}
      }
    ],
    "date": "2021-12-05",
    "categories": [
      "R",
      "simulation"
    ],
    "contents": "\n\nContents\nLet the Game begin\nMonte Carlo Solution\nBenchmarking for vs while loop\nMathematical Solution\n\n\nA few weeks ago I did an introductory R workshop where one of the students asked about loops and the difference of for vs while loops. This blogpost is about illustrating these two types of loops using a simulation example.\nLet the Game begin\nI think the majority knows the Netflix show “Squid Game”. In this series the contestants have to survive several deadly games. Here I want to focus on the 5. game of the show. There are 16 players who have to pass a bridge of 18 * 2 glass plates. At each step they have to decide on which plate they step. With a 50% chance they jump on the harder glass which is able to hold their body, the other 50% will mean their death. As I was watching the show I thought that this would be a perfect example to answer a statistical question by using loops and simulation.\nThe question I wanted to answer was:\n\nHow many players do we expect to survive the game?\n\n\n\n\nFigure 1: Source: https://www.distractify.com/p/games-played-in-squid-game\n\n\n\nRemark:\nAfter opening twitter, I came across a tweet discussing John Helveston’s blogpost where he basically explained exactly what I wanted to do. I highly recommend his blog. I adapted his run_game function and where he used data.table I went with the tidy alternative.\nMonte Carlo Solution\nWe can solve the question about the number of survivors to expect by simulating the game. When using random simulation to answer statistical problems, this is called Monte Carlo Simulation.\nFirst we load the necessary library and set a custom theme for our plots.\n\n\nlibrary(tidyverse)\nlibrary(microbenchmark)\n\nplotutils::set_custom_theme(base_size = 32)\n\n\n\nThen we create a dataframe as an input for the game. In this dataframe the alive column is set to 1 as in the beginning obviously every player is alive.\n\n\n# Define number of players\nnum_players <- 16\nplayers <- tibble(player = seq(num_players), \n                  alive = 1)\n\n# Let's have a look at the dataframe\nplayers\n\n\n# A tibble: 16 x 2\n   player alive\n    <int> <dbl>\n 1      1     1\n 2      2     1\n 3      3     1\n 4      4     1\n 5      5     1\n 6      6     1\n 7      7     1\n 8      8     1\n 9      9     1\n10     10     1\n11     11     1\n12     12     1\n13     13     1\n14     14     1\n15     15     1\n16     16     1\n\nNow we are going to create the function for our game. This is a great example to look at the differences between for and while loops.\nWe start by creating a function using a for loop:\n\n\n# Define a function for simulating one game using a for loop\nrun_game_for <- function(players, num_steps) {\n  lead_player <- 1\n  for (step in seq(num_steps)) {\n    # 50% chance that the glass is safe\n    if (sample(c(TRUE, FALSE), 1)) {\n      # It is safe, now the player can try the next one!\n      next\n    } else {\n      # The glass broke...\n      # Before continuing, check if any players are still alive\n      if (sum(players$alive) == 0) { return(0) }\n      # The lead player died\n      players$alive[lead_player] <- 0\n      lead_player <- lead_player + 1\n    }\n  }\n  return(sum(players$alive))\n}\n\n\n\nThen we create a function using a while loop. The setup is quite similar to the previously used run_game_for function.\n\n\n# Define a function for simulating one game using a while loop\nrun_game_while <- function(players, num_steps) {\n  # Initialize starting values\n  lead_player <- 1\n  current_step <- 0\n  game_running <- TRUE\n  \n  while (game_running) {\n    # Let's see if the glass holds...\n    if (sample(c(TRUE, FALSE), 1)) {\n      # The glass holds and the player can go one step further\n      current_step <- current_step + 1\n    } else {\n      # Check if there are still players alive, if not end the game\n      if (sum(players$alive) == 0) { return(0)}\n      # Apparently the glass didnt hold and the current lead player dies\n      players$alive[lead_player] <- 0\n      lead_player <- lead_player + 1\n      # Anyway the player can go one step further \n      current_step <- current_step + 1\n    }\n    if (current_step == num_steps) {\n      # If they got to the last step, they did it and the game stops\n      game_running <- FALSE\n    }\n  }\n  # Return the number of remaining players\n  return(sum(players$alive))\n}\n\n\n\nLet’s give it a try and see how many survive in our game.\n\n\nset.seed(001)\n\n# Run one iteration of the game\nsingle_game <- run_game_while(players, num_steps = 18)\nsingle_game\n\n\n[1] 9\n\nWe were interested in the expected value of the outcome. One iteration is not enough, but this is no problem at all. We can simply simulate our game multiple times.\n\n\n# Set seed value to keep reproducibility and to give a hint who wins the game\nset.seed(456)\n\n# Define number of runs or games we want to play\nn_runs <- 10000\n\n# Create dataframe with outcome of each game\nsims <- tibble(trial = seq(n_runs)) |> \n  rowwise() |> \n  mutate(while_loop = run_game_while(players, num_steps = 18),\n         for_loop = run_game_for(players, num_steps = 18)) |> \n  pivot_longer(-trial, names_to = \"loop\")\n\n\n\nHave a look at the descriptive statistics:\n\n\nsummary(sims$value)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   6.000   7.000   6.976   8.000  15.000 \n\nThere were games where zero players survived the game and there were games where almost all players managed to survive.\nOf course we can also visualize our distribution:\n\n\n# Visualizing the resulting distribution\nsims |> \n  ggplot(aes(x = value, fill = loop)) +\n  geom_bar(position = position_dodge()) +\n  scale_x_continuous(breaks = seq(0, num_players)) +\n  labs(title = \"Distribution of numbers of surviving players\",\n       x = \"N surviving players\",\n       fill = NULL)\n\n\n\n\nThe two different colours indicate which function was used to calculate the result. From this graph we directly see almost the exact same result from the two functions.\nTo answer our previously posed question: We would expect 7 players to survive the game.\nBenchmarking for vs while loop\nNow we can also test the performance of the different functions against each other.\n\n\nset.seed(001)\n\ntest <- microbenchmark(\n  run_game_for(players, num_steps = 18),\n  run_game_while(players, num_steps = 18)\n)\n\nautoplot(test)\n\n\n\n\nAgain there is not really a difference…\nMathematical Solution\nOf course we can not only simulate the game to get to our solution.\nHere is the mathematical formula for n players:\n\\[\n\\sum_{i = 0}^{n-1} \\binom{18}{i} * 0.5^i * 0.5^{18-i} * (n-i)\n\\]\nWe can convert it to R Code and calculate the result.\n\n\nn <- 16\n\nexpected_fun <- function(i) choose(18, i) * 0.5^i * 0.5^(18-i) * (n - i)\n\nmap_dbl(0:(n-1), expected_fun) |> sum()\n\n\n[1] 7.000076\n\nEt voilà! The result from the Monte Carlo simulation was confirmed.\n\n\n\n",
    "preview": "posts/2021-12-05-looping-over-the-bridge/img/squid_game_bridge.jpg",
    "last_modified": "2023-03-05T12:22:02+01:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "I started a Blog",
    "description": "Why did I start it and what are my plans?",
    "author": [
      {
        "name": "Lukas Gröninger",
        "url": {}
      }
    ],
    "date": "2021-12-03",
    "categories": [],
    "contents": "\n\nContents\nWhy did I start it?\nWhat are my plans?\n\nWhy did I start it?\nAfter reading and seeing so many great R or data science blogs I wanted to join the game. Whenever I’m searching for a solution for a problem at work or at the university, I stumble across a blog post that explains something or pushes me in the right direction. I’ve benefited soo much from various articles, posts or ideas. Additionally having a small place in the Internet sounded like a nice idea. Moreover, the R community is so friendly, welcoming and inspiring, that I thought I could participate a bit.\nWhat are my plans?\nAt the moment I don’t have any big plans for article series about certain topics. I just wanted to create something where I could share some of my work. In the best case, this blog then also serves someone to come across the solution to his or her problem.\nJust a small warning in the end. You should not expect to see only fully elaborated work and also the code might not be that elegant and efficient as seen on other great blogs (see list below).\nThis provides me with both an excuse and the motivation to get stuff online. Pls consider the whole thing as a work in progress…\nBlogs/People that inspired me:\nThe Mockup Blog\nJesse Mostipak\nJosiah Parry\nJohn Paul Helveston\nDanielle Navarro\nShannon Pileggi\nAlison Hill\nColin Fay\nand many more!\n\n\n\n",
    "preview": {},
    "last_modified": "2023-03-05T12:22:02+01:00",
    "input_file": {}
  }
]
